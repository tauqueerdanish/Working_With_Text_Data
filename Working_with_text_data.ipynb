{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tauqueerdanish/Working_With_Text_Data/blob/main/Working_with_text_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBY-IcRmnat3"
      },
      "source": [
        "# **One Hot Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EEBSwYsYnMT-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IeyNFdyAzytR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "57JCb3a81g0_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4WWucDXni5p",
        "outputId": "fdbcb265-e444-4786-962d-fc57fd5792e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ],
      "source": [
        "# Word level one hot encoding\n",
        "\n",
        "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "  for word in sample.split():\n",
        "    if word not in token_index:\n",
        "      token_index[word] = len(token_index) + 1\n",
        "\n",
        "max_length = 10\n",
        "results = np.zeros(shape = (len(samples), max_length, max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = token_index.get(word)\n",
        "    results[i, j, index] = 1.\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQft5QqMcApK",
        "outputId": "d57e5988-e34a-471e-a9ef-82a42fff9598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "-------------------------------------------------\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
            "-------------------------------------------------\n",
            "['The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.']\n",
            "-------------------------------------------------\n",
            "['T', 'h', 'e', ' ', 'c', 'a', 't', ' ', 's', 'a', 't', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'm', 'a', 't', '.', 'T', 'h', 'e', ' ', 'd', 'o', 'g', ' ', 'a', 't', 'e', ' ', 'm', 'y', ' ', 'h', 'o', 'm', 'e', 'w', 'o', 'r', 'k', '.']\n",
            "-------------------------------------------------\n",
            "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
            "-------------------------------------------------\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n"
          ]
        }
      ],
      "source": [
        "# Character level one hot encoding\n",
        "import string\n",
        "\n",
        "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
        "characters = string.printable\n",
        "token_index = dict(zip(range(1, len(characters)+1), characters))\n",
        "i2 = []\n",
        "j2 = []\n",
        "sample2 = []\n",
        "character2 = []\n",
        "index2 = []\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, character in enumerate(sample):\n",
        "    i2.append(i)\n",
        "    sample2.append(sample)\n",
        "    j2.append(j)\n",
        "    character2.append(character)\n",
        "    index = token_index.get(character)\n",
        "    index2.append(index)\n",
        "    results[i, j, index] = 1.\n",
        "\n",
        "print(i2)\n",
        "print(\"-------------------------------------------------\")\n",
        "print(j2)\n",
        "print(\"-------------------------------------------------\")\n",
        "print(sample2)\n",
        "print(\"-------------------------------------------------\")\n",
        "print(character2)\n",
        "print(\"-------------------------------------------------\")\n",
        "print(index2)\n",
        "print(\"-------------------------------------------------\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwVN2xqQecdX",
        "outputId": "32d98aa5-c337-4cbb-c855-e0c5a05c2b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 uninque tokens.\n"
          ]
        }
      ],
      "source": [
        "# Use kears for word level one hot encoding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
        "tokenizer = Tokenizer(num_words = 1000)\n",
        "tokenizer.fit_on_texts(samples)\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode=\"binary\")\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Found %s uninque tokens.\" %len(word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jpvgPEi2peaz"
      },
      "outputs": [],
      "source": [
        "# Word level one hot encoding with hashing trick\n",
        "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
        "dimensionality = 1000\n",
        "max_length = 10\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = abs(hash(word)) % dimensionality\n",
        "    results[i, j, index] = 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qoo_LWuYstmq",
        "outputId": "e8f5f433-e21a-495a-8bae-c7f9ef5d6084"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fhshYmWLswSB"
      },
      "outputs": [],
      "source": [
        "#Word embedding (no of possible tokens, dimensionality of embeddings)\n",
        "embedding_layer = Embedding(1000, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvqpzghtvdFK",
        "outputId": "61c2d389-e73a-47fb-c1b0-96e640d6befa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#Loading imdb dataset for working with word embedding\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 20\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n",
        "\n",
        "#Turns the list of integers into 2d integer tensor of shape(samples, maxlen)\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vUjXPwnx5Tf",
        "outputId": "ac040e3a-0d40-4da4-d76d-919134a90e99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 20, 8)             80000     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 160)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 161       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.6648 - acc: 0.6361 - val_loss: 0.6090 - val_acc: 0.7000\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.5352 - acc: 0.7538 - val_loss: 0.5230 - val_acc: 0.7320\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 2ms/step - loss: 0.4579 - acc: 0.7898 - val_loss: 0.4962 - val_acc: 0.7496\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4189 - acc: 0.8101 - val_loss: 0.4920 - val_acc: 0.7560\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3918 - acc: 0.8272 - val_loss: 0.4919 - val_acc: 0.7616\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3696 - acc: 0.8376 - val_loss: 0.4952 - val_acc: 0.7564\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 2ms/step - loss: 0.3498 - acc: 0.8488 - val_loss: 0.5000 - val_acc: 0.7576\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3313 - acc: 0.8600 - val_loss: 0.5058 - val_acc: 0.7572\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3139 - acc: 0.8703 - val_loss: 0.5140 - val_acc: 0.7552\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2968 - acc: 0.8788 - val_loss: 0.5231 - val_acc: 0.7508\n"
          ]
        }
      ],
      "source": [
        "#Using and embedding layer and classifier on the IMDB dataset\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(\n",
        "    optimizer =\"rmsprop\",\n",
        "    loss = \"binary_crossentropy\",\n",
        "    metrics = [\"acc\"]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs = 10,\n",
        "    batch_size = 32,\n",
        "    validation_split = 0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AtO0mmSs0Oub"
      },
      "outputs": [],
      "source": [
        "#Using prerained word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "i64vQ7nouB1g"
      },
      "outputs": [],
      "source": [
        "# First we will extract the folder\n",
        "#shutil.unpack_archive(\"/content/drive/MyDrive/aclImdb.zip\", \"/content/drive/MyDrive/aclImdb_1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3WPJTJon0vTD"
      },
      "outputs": [],
      "source": [
        "imdb_dir = \"/content/drive/MyDrive/aclImdb_1/aclImdb\"\n",
        "train_dir = os.path.join(imdb_dir,\"train\")\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in [\"neg\",\"pos\"]:\n",
        "  dir_name = os.path.join(train_dir, label_type)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    if fname[-4:] == \".txt\":\n",
        "        f = open(os.path.join(dir_name, fname))\n",
        "        texts.append(f.read())\n",
        "        f.close()\n",
        "        if label_type == \"neg\":\n",
        "            labels.append(0)\n",
        "        else:\n",
        "            labels.append(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UkHIhLzZ7Cpf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c84bac-d5e6-47dc-cdc1-a205bcecb605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fount 52655 unique tokens.\n",
            "Shape of data tensor: (2, 100)\n",
            "shape of label tensor: (8770,)\n"
          ]
        }
      ],
      "source": [
        "#We use only first 200 samples because we are using the pretrained embedding\n",
        "\n",
        "#cutoff the reviews after 100 words\n",
        "maxlen = 100\n",
        "training_samples = 200   #take only 200 samples\n",
        "validation_samples = 10000   #validate on 10,000 samples\n",
        "max_words = 10000       #considers only the 10,000 top words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words= max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequence = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Fount %s unique tokens.\"%len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = np.asarray(labels)\n",
        "print(\"Shape of data tensor:\", data.shape)\n",
        "print(\"shape of label tensor:\", labels.shape)\n",
        "\n",
        "#Now, we should have tosplit the data into training and validation set. so, first we will shuffle the data because \n",
        "# data is ordered and negative are on first and positive are on second number.\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[: training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IirZsXm4Czgm"
      },
      "outputs": [],
      "source": [
        "# shutil.unpack_archive(\"/content/drive/MyDrive/glove.6B.zip\", \"/content/drive/MyDrive/glove.6B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6owWku0Um9cj",
        "outputId": "39ba944d-a3d8-48e5-fce3-20a4ba41ec0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "glove_dir = \"/content/drive/MyDrive/glove.6B\"\n",
        "\n",
        "embeddings_index = {}\n",
        "\n",
        "f=open(os.path.join(glove_dir, \"glove.6B.100d.txt\"))\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print(\"Found %s word vectors.\" %len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Vh7eG0imK39t"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  if i<max_words:\n",
        "    embedding_vector= embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo4upHHSL359",
        "outputId": "6fa64c0e-e35f-4e37-8e72-8efefe1c95a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 10000)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                320032    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,320,065\n",
            "Trainable params: 1,320,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QLrvsIuG2EoP"
      },
      "outputs": [],
      "source": [
        "# loading pretrained word embeddings into the embedding layer\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdkfc9PD20sX",
        "outputId": "61f1ba31-ed0e-4624-803b-917f2835cda4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 0.6149 - acc: 1.0000\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.1007 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0568 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0400 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.0307 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0247 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0205 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0174 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0150 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0131 - acc: 1.0000\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history= model.fit(x_train, y_train,\n",
        "                   epochs=10,\n",
        "                   batch_size=32,\n",
        "                   validation_data=(x_val, y_val))\n",
        "model.save_weights('pre_trained_glove_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "r8ZKhTG-60hw",
        "outputId": "4e794fce-d1ea-4d21-dc53-ed7241fb180c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': [0.6149299144744873, 0.1007232740521431, 0.05679283291101456, 0.03998789191246033, 0.03068557195365429, 0.02470245584845543, 0.02051052823662758, 0.01740427315235138, 0.015009034425020218, 0.013106169179081917], 'acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-3fb37e979c1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'validation_acc'"
          ]
        }
      ],
      "source": [
        "print(history.history)\n",
        "acc= history.history['acc']\n",
        "val_acc = history.history['validation_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pavr8bmYDtME"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Working_with_text_data.ipynb",
      "provenance": [],
      "mount_file_id": "1TvcPKkEa0wFjUcurhZLvTV00MCC5P-tt",
      "authorship_tag": "ABX9TyPd1DOYZ4HeqkKZ6NjmKc8k",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
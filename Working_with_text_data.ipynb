{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Working_with_text_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1TvcPKkEa0wFjUcurhZLvTV00MCC5P-tt",
      "authorship_tag": "ABX9TyOktAJpeQGg3vJ/iyaUunPd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tauqueerdanish/Working_With_Text_Data/blob/main/Working_with_text_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBY-IcRmnat3"
      },
      "source": [
        "# **One Hot Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEBSwYsYnMT-"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "IeyNFdyAzytR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4WWucDXni5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993c63e2-6fc0-4b9b-9474-c35eb0a815ae"
      },
      "source": [
        "# Word level one hot encoding\n",
        "\n",
        "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "  for word in sample.split():\n",
        "    if word not in token_index:\n",
        "      token_index[word] = len(token_index) + 1\n",
        "\n",
        "max_length = 10\n",
        "results = np.zeros(shape = (len(samples), max_length, max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = token_index.get(word)\n",
        "    results[i, j, index] = 1.\n",
        "print(results)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQft5QqMcApK",
        "outputId": "a02105a9-f55d-48d1-9ee9-d9eb19c64380"
      },
      "source": [
        "# Character level one hot encoding\n",
        "import string\n",
        "\n",
        "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
        "characters = string.printable\n",
        "token_index = dict(zip(range(1, len(characters)+1), characters))\n",
        "i2 = []\n",
        "j2 = []\n",
        "sample2 = []\n",
        "character2 = []\n",
        "index2 = []\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, character in enumerate(sample):\n",
        "    i2.append(i)\n",
        "    sample2.append(sample)\n",
        "    j2.append(j)\n",
        "    character2.append(character)\n",
        "    index = token_index.get(character)\n",
        "    index2.append(index)\n",
        "    results[i, j, index] = 1.\n",
        "\n",
        "print(i2)\n",
        "print(\"-------------------------------------------------\")\n",
        "print(j2)\n",
        "print(\"-------------------------------------------------\")\n",
        "print(sample2)\n",
        "print(\"-------------------------------------------------\")\n",
        "print(character2)\n",
        "print(\"-------------------------------------------------\")\n",
        "print(index2)\n",
        "print(\"-------------------------------------------------\")\n",
        "print(results)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "-------------------------------------------------\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
            "-------------------------------------------------\n",
            "['The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The cat sat on the mat.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.', 'The dog ate my homework.']\n",
            "-------------------------------------------------\n",
            "['T', 'h', 'e', ' ', 'c', 'a', 't', ' ', 's', 'a', 't', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'm', 'a', 't', '.', 'T', 'h', 'e', ' ', 'd', 'o', 'g', ' ', 'a', 't', 'e', ' ', 'm', 'y', ' ', 'h', 'o', 'm', 'e', 'w', 'o', 'r', 'k', '.']\n",
            "-------------------------------------------------\n",
            "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
            "-------------------------------------------------\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwVN2xqQecdX",
        "outputId": "2ce34caf-6fe4-43fe-b011-ab3435858473"
      },
      "source": [
        "# Use kears for word level one hot encoding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
        "tokenizer = Tokenizer(num_words = 1000)\n",
        "tokenizer.fit_on_texts(samples)\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode=\"binary\")\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Found %s uninque tokens.\" %len(word_index))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 uninque tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpvgPEi2peaz"
      },
      "source": [
        "# Word level one hot encoding with hashing trick\n",
        "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
        "dimensionality = 1000\n",
        "max_length = 10\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = abs(hash(word)) % dimensionality\n",
        "    results[i, j, index] = 1."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qoo_LWuYstmq",
        "outputId": "9565ad3a-d9e8-4c75-8389-3aa3bf20a532"
      },
      "source": [
        "results"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhshYmWLswSB"
      },
      "source": [
        "#Word embedding (no of possible tokens, dimensionality of embeddings)\n",
        "embedding_layer = Embedding(1000, 64)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvqpzghtvdFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cc64a0e-9c9e-4135-d090-82b5b0a31cf0"
      },
      "source": [
        "#Loading imdb dataset for working with word embedding\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 20\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n",
        "\n",
        "#Turns the list of integers into 2d integer tensor of shape(samples, maxlen)\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vUjXPwnx5Tf",
        "outputId": "1cf51060-ef2f-4d59-ed55-0068e320690d"
      },
      "source": [
        "#Using and embedding layer and classifier on the IMDB dataset\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(\n",
        "    optimizer =\"rmsprop\",\n",
        "    loss = \"binary_crossentropy\",\n",
        "    metrics = [\"acc\"]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs = 10,\n",
        "    batch_size = 32,\n",
        "    validation_split = 0.2\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 20, 8)             80000     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 160)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 161       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 2s 2ms/step - loss: 0.6696 - acc: 0.6175 - val_loss: 0.6209 - val_acc: 0.6898\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.5427 - acc: 0.7530 - val_loss: 0.5267 - val_acc: 0.7330\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4608 - acc: 0.7892 - val_loss: 0.4994 - val_acc: 0.7486\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4204 - acc: 0.8101 - val_loss: 0.4930 - val_acc: 0.7498\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3926 - acc: 0.8232 - val_loss: 0.4943 - val_acc: 0.7540\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3694 - acc: 0.8384 - val_loss: 0.4974 - val_acc: 0.7554\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 2ms/step - loss: 0.3491 - acc: 0.8518 - val_loss: 0.5020 - val_acc: 0.7548\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 2ms/step - loss: 0.3298 - acc: 0.8605 - val_loss: 0.5083 - val_acc: 0.7532\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3122 - acc: 0.8700 - val_loss: 0.5139 - val_acc: 0.7544\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.2953 - acc: 0.8794 - val_loss: 0.5226 - val_acc: 0.7504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtO0mmSs0Oub"
      },
      "source": [
        "#Using prerained word embeddings"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First we will extract the folder\n",
        "#shutil.unpack_archive(\"/content/drive/MyDrive/aclImdb.zip\", \"/content/drive/MyDrive/aclImdb_1\")"
      ],
      "metadata": {
        "id": "i64vQ7nouB1g"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_dir = \"/content/drive/MyDrive/aclImdb_1/aclImdb\"\n",
        "train_dir = os.path.join(imdb_dir,\"train\")\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in [\"neg\",\"pos\"]:\n",
        "  dir_name = os.path.join(train_dir, label_type)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    if fname[-4:] == \".txt\":\n",
        "        f = open(os.path.join(dir_name, fname))\n",
        "        texts.append(f.read())\n",
        "        f.close()\n",
        "        if label_type == \"neg\":\n",
        "            labels.append(0)\n",
        "        else:\n",
        "            labels.append(1)\n"
      ],
      "metadata": {
        "id": "3WPJTJon0vTD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We use only first 200 samples because we are using the pretrained embedding\n",
        "\n",
        "#cutoff the reviews after 100 words\n",
        "maxlen = 100\n",
        "training_samples = 200   #take only 200 samples\n",
        "validation_samples = 10000   #validate on 10,000 samples\n",
        "max_words = 10000       #considers only the 10,000 top words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words= max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequence = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Fount %s unique tokens.\"%len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = np.asarray(labels)\n",
        "print(\"Shape of data tensor:\", data.shape)\n",
        "print(\"shape of label tensor:\", labels.shape)\n",
        "\n",
        "#Now, we should have tosplit the data into training and validation set. so, first we will shuffle the data because \n",
        "# data is ordered and negative are on first and positive are on second number.\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = data[: training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = data[training_samples: training_samples + validation_samples]"
      ],
      "metadata": {
        "id": "UkHIhLzZ7Cpf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d139d58c-5f15-4755-eb8a-1d7ea162ed1e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fount 88582 unique tokens.\n",
            "Shape of data tensor: (2, 100)\n",
            "shape of label tensor: (25000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shutil.unpack_archive(\"/content/drive/MyDrive/glove.6B.zip\", \"/content/drive/MyDrive/glove.6B\")"
      ],
      "metadata": {
        "id": "IirZsXm4Czgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_dir = \"/content/drive/MyDrive/glove.6B\"\n",
        "\n",
        "embeddings_index = {}\n",
        "\n",
        "f=open(os.path.join(glove_dir, \"glove.6B.100d.txt\"))\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print(\"Found %s word vectors.\" %len(embeddings_index))"
      ],
      "metadata": {
        "id": "6owWku0Um9cj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "732ddd59-565f-4f72-f9c0-c51734a080ca"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  if i<max_words:\n",
        "    embedding_vector= embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "Vh7eG0imK39t"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Qo4upHHSL359"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}